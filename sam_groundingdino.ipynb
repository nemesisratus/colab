{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 1 - Environment setup (stable versions)\n",
        "\n",
        "# Install PyTorch (CUDA 12.1 build) - works on Colab GPUs\n",
        "!pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# Other dependencies\n",
        "!pip install opencv-python matplotlib tqdm\n",
        "\n",
        "# Install HuggingFace and other utilities\n",
        "!pip install git+https://github.com/facebookresearch/segment-anything.git\n",
        "!pip install git+https://github.com/IDEA-Research/GroundingDINO.git\n",
        "!pip install huggingface_hub\n",
        "\n",
        "# Make sure ffmpeg is installed\n",
        "!apt-get update -qq\n",
        "!apt-get install -y ffmpeg\n",
        "\n",
        "# Clone repos (for models and scripts)\n",
        "!git clone https://github.com/facebookresearch/segment-anything.git\n",
        "!git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
        "\n",
        "print(\"✅ Environment ready. Next step: download model weights\")\n"
      ],
      "metadata": {
        "id": "lhZOdqE0qpV7",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 2 - Download model weights for GroundingDINO and SAM\n",
        "\n",
        "import os\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(\"weights\", exist_ok=True)\n",
        "\n",
        "# GroundingDINO weights\n",
        "if not os.path.exists(\"weights/groundingdino_swint_ogc.pth\"):\n",
        "    !wget -O weights/groundingdino_swint_ogc.pth https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\n",
        "\n",
        "# SAM weights (ViT-H, most accurate)\n",
        "if not os.path.exists(\"weights/sam_vit_h_4b8939.pth\"):\n",
        "    !wget -O weights/sam_vit_h_4b8939.pth https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
        "\n",
        "print(\"✅ Model weights downloaded and stored in ./weights\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pCW9_LGerbUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 3 - Upload video and inspect properties\n",
        "import os\n",
        "from google.colab import files\n",
        "import subprocess\n",
        "\n",
        "# Upload video\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get uploaded filename\n",
        "video_path = list(uploaded.keys())[0]\n",
        "print(f\"✅ Uploaded video: {video_path}\")\n",
        "\n",
        "# Ambil nama file yang diupload (anggap hanya satu file)\n",
        "input_video = list(uploaded.keys())[0]\n",
        "output_video = \"output.mp4\"\n",
        "!ffmpeg -i \"$input_video\" -filter:v \"fps=12\" \"$output_video\" -y\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Inspect video with ffmpeg\n",
        "print(\"\\n--- Video Properties ---\")\n",
        "subprocess.run([\"ffmpeg\", \"-i\", video_path, \"-hide_banner\"])\n"
      ],
      "metadata": {
        "id": "oLpAZKd6sR7U",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 4 - Extract frames and audio from the input video\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Create folders\n",
        "os.makedirs(\"frames\", exist_ok=True)\n",
        "video12fps = \"/content/output.mp4\"\n",
        "# Extract frames (as PNG to avoid compression artifacts)\n",
        "subprocess.run([\n",
        "    \"ffmpeg\", \"-i\", video12fps,\n",
        "    \"-q:v\", \"2\",  # high quality\n",
        "    \"frames/frame_%06d.png\"\n",
        "])\n",
        "\n",
        "print(\"✅ Frames extracted to ./frames\")\n",
        "\n",
        "# Extract audio (if exists)\n",
        "if not os.path.exists(\"audio.aac\"):\n",
        "    subprocess.run([\n",
        "        \"ffmpeg\", \"-i\", video12fps,\n",
        "        \"-vn\", \"-acodec\", \"copy\", \"audio.aac\"\n",
        "    ])\n",
        "    print(\"✅ Audio extracted to audio.aac\")\n",
        "else:\n",
        "    print(\"⚠️ Audio already exists, skipped extraction\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "QCaXD2dfstxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 5 - Run GroundingDINO on frames to produce detections\n",
        "\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "\n",
        "# === Prompt input ===\n",
        "prompt = 'dress'  #@param {type: \"string\"}\n",
        "box_threshold = 0.3  #@param {type: \"number\"}\n",
        "text_threshold = 0.25\n",
        "\n",
        "# Import GroundingDINO\n",
        "import sys\n",
        "sys.path.append(\"GroundingDINO\")\n",
        "\n",
        "from groundingdino.util.inference import load_model, load_image, predict\n",
        "\n",
        "# Load model\n",
        "model_path = \"weights/groundingdino_swint_ogc.pth\"\n",
        "config_path = \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
        "dino_model = load_model(config_path, model_path)\n",
        "\n",
        "# Prepare output folder\n",
        "os.makedirs(\"detections\", exist_ok=True)\n",
        "\n",
        "# Process all frames\n",
        "frame_files = sorted([f for f in os.listdir(\"frames\") if f.endswith(\".png\")])\n",
        "print(f\"Processing {len(frame_files)} frames with prompt: {prompt}\")\n",
        "\n",
        "for frame_file in tqdm(frame_files):\n",
        "    frame_path = os.path.join(\"frames\", frame_file)\n",
        "    image_source, image = load_image(frame_path)  # image_source = np.array, image = tensor\n",
        "\n",
        "    # Run prediction\n",
        "    boxes, logits, phrases = predict(\n",
        "        model=dino_model,\n",
        "        image=image,\n",
        "        caption=prompt,\n",
        "        box_threshold=box_threshold,\n",
        "        text_threshold=text_threshold\n",
        "    )\n",
        "\n",
        "    h, w, _ = image_source.shape\n",
        "\n",
        "    # Convert normalized [cx, cy, w, h] → absolute [x1, y1, x2, y2]\n",
        "    abs_boxes = []\n",
        "    for (cx, cy, bw, bh) in boxes.tolist():\n",
        "        x1 = (cx - bw/2) * w\n",
        "        y1 = (cy - bh/2) * h\n",
        "        x2 = (cx + bw/2) * w\n",
        "        y2 = (cy + bh/2) * h\n",
        "        abs_boxes.append([x1, y1, x2, y2])\n",
        "\n",
        "    # Save detection metadata\n",
        "    det_path = os.path.join(\"detections\", frame_file.replace(\".png\", \".json\"))\n",
        "    with open(det_path, \"w\") as f:\n",
        "        json.dump({\n",
        "            \"boxes\": abs_boxes,\n",
        "            \"phrases\": phrases,\n",
        "            \"logits\": logits.tolist()\n",
        "        }, f)\n",
        "\n",
        "print(\"✅ All detections saved in ./detections as absolute pixel coords\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "r87Qtx2htli7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 5.5 – Preview bounding boxes from detections 12fps(Video)\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Prepare output folder\n",
        "os.makedirs(\"previews\", exist_ok=True)\n",
        "\n",
        "# Get frame list\n",
        "frame_files = sorted([f for f in os.listdir(\"frames\") if f.endswith(\".png\")])\n",
        "\n",
        "# Read first frame to set video size\n",
        "first_frame = cv2.imread(os.path.join(\"frames\", frame_files[0]))\n",
        "h, w = first_frame.shape[:2]\n",
        "\n",
        "# Create video writer (MP4, 30 fps)\n",
        "out_path = \"previews/preview_boxes.mp4\"\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "video = cv2.VideoWriter(out_path, fourcc, 12, (w, h))\n",
        "\n",
        "for frame_file in tqdm(frame_files):\n",
        "    frame_path = os.path.join(\"frames\", frame_file)\n",
        "    det_path = os.path.join(\"detections\", frame_file.replace(\".png\", \".json\"))\n",
        "\n",
        "    # Load frame\n",
        "    image = cv2.imread(frame_path)\n",
        "\n",
        "    # If no detection JSON, just write raw frame\n",
        "    if not os.path.exists(det_path):\n",
        "        video.write(image)\n",
        "        continue\n",
        "\n",
        "    # Load detections\n",
        "    with open(det_path, \"r\") as f:\n",
        "        det = json.load(f)\n",
        "    boxes = np.array(det[\"boxes\"])\n",
        "\n",
        "    # Draw boxes\n",
        "    if boxes.size > 0:\n",
        "        for box, phrase in zip(boxes.astype(int), det.get(\"phrases\", [])):\n",
        "            x1, y1, x2, y2 = box\n",
        "            cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "            cv2.putText(image, phrase, (x1, max(y1 - 5, 0)),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
        "\n",
        "    # Write frame to video\n",
        "    video.write(image)\n",
        "\n",
        "video.release()\n",
        "print(f\"✅ Video saved: {out_path}\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"/content/previews/preview_boxes.mp4\")\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "OkL6kOOd7i-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 6 - Run SAM on detections to generate masks\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# Import SAM\n",
        "import sys\n",
        "sys.path.append(\"segment-anything\")\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "# Load SAM model\n",
        "sam_checkpoint = \"weights/sam_vit_h_4b8939.pth\"\n",
        "sam = sam_model_registry[\"vit_h\"](checkpoint=sam_checkpoint)\n",
        "sam.to(\"cuda\")\n",
        "predictor = SamPredictor(sam)\n",
        "\n",
        "# Prepare mask folder\n",
        "os.makedirs(\"masks\", exist_ok=True)\n",
        "\n",
        "# Process frames\n",
        "frame_files = sorted([f for f in os.listdir(\"frames\") if f.endswith(\".png\")])\n",
        "\n",
        "saved_count = 0\n",
        "skipped_count = 0\n",
        "\n",
        "for frame_file in tqdm(frame_files):\n",
        "    frame_path = os.path.join(\"frames\", frame_file)\n",
        "    det_path = os.path.join(\"detections\", frame_file.replace(\".png\", \".json\"))\n",
        "\n",
        "    # Skip if no detections\n",
        "    if not os.path.exists(det_path):\n",
        "        skipped_count += 1\n",
        "        continue\n",
        "\n",
        "    # Load frame\n",
        "    image = cv2.imread(frame_path)\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    predictor.set_image(image_rgb)\n",
        "\n",
        "    # Load detections\n",
        "    with open(det_path, \"r\") as f:\n",
        "        det = json.load(f)\n",
        "    boxes = np.array(det[\"boxes\"])\n",
        "\n",
        "    if len(boxes) == 0:\n",
        "        # Save empty mask if nothing detected\n",
        "        mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
        "    else:\n",
        "        # Predict mask for each box\n",
        "        transformed_boxes = predictor.transform.apply_boxes_torch(\n",
        "            torch.tensor(boxes, device=\"cuda\"),\n",
        "            image.shape[:2]\n",
        "        )\n",
        "        masks, _, _ = predictor.predict_torch(\n",
        "            point_coords=None,\n",
        "            point_labels=None,\n",
        "            boxes=transformed_boxes,\n",
        "            multimask_output=False\n",
        "        )\n",
        "\n",
        "        # Merge all masks into one binary mask\n",
        "        mask = masks.sum(dim=0).cpu().numpy()\n",
        "        mask = (mask > 0).astype(np.uint8) * 255\n",
        "\n",
        "    # Fix shape (remove leading channel if exists)\n",
        "    mask = np.squeeze(mask)\n",
        "\n",
        "    # Save mask with debug check\n",
        "    mask_path = os.path.join(\"masks\", frame_file)\n",
        "    success = cv2.imwrite(mask_path, mask)\n",
        "    if not success:\n",
        "        print(\"❌ Failed to save:\", mask_path, \"| shape:\", mask.shape, \"| dtype:\", mask.dtype, \"| unique:\", np.unique(mask))\n",
        "    else:\n",
        "        saved_count += 1\n",
        "\n",
        "print(f\"✅ All masks processed. Saved: {saved_count}, Skipped: {skipped_count}, Total frames: {len(frame_files)}\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lABeur01ubM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 7 - Generate masked frames for Output A (B&W) and Output B (Grey overlay)\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Prepare output folders\n",
        "os.makedirs(\"output_bw\", exist_ok=True)\n",
        "os.makedirs(\"output_grey\", exist_ok=True)\n",
        "\n",
        "frame_files = sorted([f for f in os.listdir(\"frames\") if f.endswith(\".png\")])\n",
        "\n",
        "for frame_file in tqdm(frame_files):\n",
        "    frame_path = os.path.join(\"frames\", frame_file)\n",
        "    mask_path = os.path.join(\"masks\", frame_file)\n",
        "\n",
        "    # Load images\n",
        "    frame = cv2.imread(frame_path)\n",
        "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    if mask is None:\n",
        "        # if no mask found, create empty mask\n",
        "        mask = np.zeros(frame.shape[:2], dtype=np.uint8)\n",
        "\n",
        "    # --- Expand mask (without blur for BW output) ---\n",
        "    iterations = 20   # expand ~20px\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
        "    expanded = cv2.dilate(mask, kernel, iterations=iterations)\n",
        "\n",
        "    # --- Output A: black & white mask (expanded, not blurred) ---\n",
        "    bw = cv2.merge([expanded, expanded, expanded])  # 3-channel B&W\n",
        "    cv2.imwrite(os.path.join(\"output_bw\", frame_file), bw)\n",
        "\n",
        "    # --- Smooth expanded mask for blending in grey overlay ---\n",
        "    smooth = cv2.GaussianBlur(expanded, (41, 41), 0)\n",
        "    smooth = cv2.normalize(smooth, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "\n",
        "    # --- Output B: grey overlay ---\n",
        "    grey = frame.copy()\n",
        "    grey_color = (126, 126, 126)  # mid-grey\n",
        "\n",
        "    # Create alpha mask for blending\n",
        "    alpha = smooth.astype(float) / 255.0\n",
        "    for c in range(3):\n",
        "        grey[:, :, c] = (1 - alpha) * grey[:, :, c] + alpha * grey_color[c]\n",
        "\n",
        "    # Apply blur only to output_grey\n",
        "    grey = cv2.GaussianBlur(grey, (9, 9), 0)\n",
        "\n",
        "    cv2.imwrite(os.path.join(\"output_grey\", frame_file), grey)\n",
        "\n",
        "print(\"✅ Output frames saved in ./output_bw (expanded) and ./output_grey (expanded + blurred)\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TO-zkSyMu9w9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 8 - Re-encode frames into videos and reattach audio\n",
        "\n",
        "import subprocess\n",
        "import re\n",
        "\n",
        "# Rebuild Output A (B&W)\n",
        "subprocess.run([\n",
        "    \"ffmpeg\", \"-y\", \"-framerate\", \"12\", \"-i\", \"output_bw/frame_%06d.png\",\n",
        "    \"-i\", \"audio.aac\", \"-c:v\", \"libx264\", \"-pix_fmt\", \"yuv420p\",\n",
        "    \"-c:a\", \"aac\", \"-shortest\", \"output_bw.mp4\"\n",
        "])\n",
        "\n",
        "# Rebuild Output B (Grey overlay)\n",
        "subprocess.run([\n",
        "    \"ffmpeg\", \"-y\", \"-framerate\", \"12\", \"-i\", \"output_grey/frame_%06d.png\",\n",
        "    \"-i\", \"audio.aac\", \"-c:v\", \"libx264\", \"-pix_fmt\", \"yuv420p\",\n",
        "    \"-c:a\", \"aac\", \"-shortest\", \"output_grey.mp4\"\n",
        "])\n",
        "\n",
        "print(\"✅ Videos created: output_bw.mp4 and output_grey.mp4\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "l55VgD1bvP3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 9 Download\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"/content/output_bw.mp4\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"/content/output_grey.mp4\")"
      ],
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "H-U5ECnvwNUl",
        "outputId": "bced75f3-955e-48cb-92a6-8194d6177489"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_57a6ed44-6b56-42ce-9b0f-71c07a51baaf\", \"output_bw.mp4\", 406484)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_73a655d9-2ec9-422e-8ab4-c11bcc481a2e\", \"output_grey.mp4\", 770789)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Clear All\n",
        "import shutil\n",
        "\n",
        "shutil.rmtree('/content/frames')\n",
        "shutil.rmtree('/content/detections')\n",
        "shutil.rmtree('/content/masks')\n",
        "shutil.rmtree('/content/output_bw')\n",
        "shutil.rmtree('/content/output_grey')"
      ],
      "metadata": {
        "id": "LVty91WsNYqw",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}